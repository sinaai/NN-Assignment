{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wx0DEHvrwYji"
   },
   "source": [
    "# Multilayer Perceptron\n",
    "\n",
    "\n",
    "Neural Network / Fall 1399, Iran University of Science and Technology\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJQpO_5YOgf5"
   },
   "source": [
    "## 1. MLP from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ea3v_7iWOr-N"
   },
   "source": [
    "In this assignment, you will explore and implement the properties of a primary deep learning model called ***multilayer perceptron(MLP)***. Basically, the goal of an MLP is to learn a non-linear mapping from inputs to outputs. We can show this mapping as $y = f(x; \\theta)$ , where $x$ is the input and $\\theta$ is a vector of all the parameters in the network, which we're trying to learn.  \n",
    "\n",
    "\n",
    "As you see in the figure, every MLP network consists of an input layer, an output layer, and one or more hidden layers in between. Each layer consists of one or more cells called Neurons. In every Neuron, a dot product between the inputs of the cell and a weight vector is calculated. The result of the dot product then goes through a non-linear function (activation function e.g. $tanh$ or $sigmoid$) and gives us the output of the neuron.\n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=https://static.javatpoint.com/tutorial/tensorflow/images/multi-layer-perceptron-in-tensorflow.png width=\"500\" align=\"center\">\n",
    "</center>\n",
    "\n",
    "\n",
    "<br>\n",
    "Thoughout this assignment, inputs will be matrices with the shape of $b \\times M$ where $b$ is the batch size and $M$ is the number of features of inputs. <br>\n",
    "As for the equations, let's compute the output of the $i$th layer:\n",
    "$$A^i = f(A^{i-1}w^i + b^i)$$\n",
    "\n",
    "Imagine that $(i-1)$th and $i$th layer have sizes of $n$ and $p$ respectively. The dimensions of weight and bias will be as follows:\n",
    "<br><br>\n",
    "$$w^{n\\times p} , b^{1\\times p}$$\n",
    " <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "87vFrWhN3bub"
   },
   "source": [
    "Numpy is the only package you're allowed to use for implementing your MLP in this assignment, so let's import it in the cell below! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8gvunZcHgOe_"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uvoXV-1lLXu-"
   },
   "source": [
    "### 1.1 Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mpfexSvg3yoK"
   },
   "source": [
    "Now let's implement some activation functions! Linear, Relu and Sigmoid are the functions that we'll need in this assignment. Note that you should also implement their derivatives since you'll need them later for back-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tfbJgD2GCdm"
   },
   "outputs": [],
   "source": [
    "## We've implemented the Linear activation function for you\n",
    "\n",
    "def linear(x, deriv=False):\n",
    "\n",
    "  return x if not deriv else np.ones_like(x)\n",
    "\n",
    "def relu(x, deriv=False):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    x: A numpy array of any shape \n",
    "    deriv: True or False. determines if we want the derivative of the function or not.\n",
    "    \n",
    "  Returns:\n",
    "    relu_out: A numpy array of the same shape as x. \n",
    "      Basically relu function or its derivative applied to every element of x\n",
    "               \n",
    "  \"\"\"\n",
    "\n",
    "  ########################################\n",
    "  #     Put your implementation here     #\n",
    "  ######################################## \n",
    "  \n",
    "  \n",
    "  \n",
    "  return relu_out\n",
    "  \n",
    "def  sigmoid(x, deriv=False):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    x: A numpy array of any shape \n",
    "    deriv: True or False. determines if we want the derivative of the function or not.\n",
    "    \n",
    "  Returns:\n",
    "    sig_out: A numpy array of the same shape as x. \n",
    "      Basically sigmoid function or its derivative applied to every element of x\n",
    "               \n",
    "  \"\"\"\n",
    "\n",
    "  ########################################\n",
    "  #     Put your implementation here     #\n",
    "  ########################################\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  return sig_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "70XLtOxeKEV0"
   },
   "source": [
    "**Question**: Why do activation functions have to be non-linear? Could any non-linear function be used as an activation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6WI7RhEkK_fK"
   },
   "source": [
    "<font color=red>Write your answers here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2DMSRxfLwkz"
   },
   "source": [
    "### 1.2 Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sRyGqmxXMkh-"
   },
   "source": [
    "Now let's implement our MLP class. This class handles adding layers and doing the forward propagation. Here are the attributes of this class:\n",
    "<br> -  __parameters__: A list of dictionaries in the form of _{'w': weight, 'b': bias}_ where _weight_ and _bias_ are weight matrix and bias vector of a layer.\n",
    "<br>- __act_funcs__: A list of activation functions used in the corresponding layer.\n",
    "<br>- __activations__: A list of matrices each corresponding to the output of each layer.\n",
    "<br>- __weighted_ins__: A list of matrices each corresponding to the weighted input of each layer. Weighted input, as the name suggests, is layer's input multiplied by layer's weights and added to layer's bias. Which then goes into the layer's activation function to compute the layer's activations(outputs)!\n",
    "<br> Note that we store weighted inputs and outputs of the layers because we'll need them later for implementing the back-propagation algorithm. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0LSOmEAgONii"
   },
   "source": [
    "You only need to complete the _feed_forward_ function in the MLP class. This function performs forward propagation on the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvgYrWCNj11P"
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "\n",
    "  def __init__(self, input_dim):\n",
    "    \"\"\"\n",
    "  Args:\n",
    "    input_dim: An integer determining the inpu dimension of the MLP\n",
    "               \n",
    "  \"\"\"\n",
    "\n",
    "    self.input_dim = input_dim\n",
    "    self.parameters = []\n",
    "    self.act_funcs = []\n",
    "    self.activations = []\n",
    "    self.weighted_ins = []\n",
    "\n",
    "  def add_layer(self, layer_size, act_func=linear):\n",
    "    \"\"\"\n",
    "    Add layers to the MLP using this function\n",
    "  Args:\n",
    "    layer_size: An integer determinig the number of neurons in the layer\n",
    "    act_func: A function applied to the units in the layer \n",
    "    \n",
    "               \n",
    "  \"\"\"\n",
    "    ### Size of the previous layer of mlp\n",
    "    prev_size = self.input_dim if not self.parameters else self.parameters[-1]['w'].shape[-1]\n",
    "\n",
    "    ### Weight scale used in He initialization\n",
    "    weight_scale = np.sqrt(2/prev_size)\n",
    "    ### initializing the weights and bias of the layer\n",
    "    weight = np.random.normal(size=(prev_size, layer_size))*weight_scale\n",
    "    bias = np.ones(layer_size) *0.1\n",
    "    ### Add weights and bias of the layer to the parameters of the MLP\n",
    "    self.parameters.append({'w': weight, 'b': bias})\n",
    "    ### Add the layer's activation function \n",
    "    self.act_funcs.append(act_func)\n",
    "\n",
    "\n",
    "\n",
    "  def feed_forward(self, X):\n",
    "    \"\"\"\n",
    "    Propagate the inputs forward using this function\n",
    "  Args:\n",
    "    X: A numpy array of shape (b, input_dim) where b is the batch size and input_dim is the dimension of the input\n",
    "    \n",
    "  Returns:\n",
    "    mlp_out: A numpy array of shape (b, out_dim) where b is the batch size and out_dim is the dimension of the output\n",
    "\n",
    "    Hint: Don't forget to store weighted inputs and outputs of each layer in self.weighted_ins and self.activations respectively\n",
    "               \n",
    "  \"\"\"\n",
    "    self.activations = []\n",
    "    self.weighted_ins = []\n",
    "    mlp_out = X\n",
    "    ########################################\n",
    "    #     Put your implementation here     #\n",
    "    ######################################## \n",
    "    \n",
    "    \n",
    "\n",
    "    return mlp_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kx2D_UjSs7Ly"
   },
   "source": [
    "__Question__: In the _add_layer_ function of the MLP class, we used a method called _He initialization_ to initialize the weights. Explain how this method can help with the training of an MLP?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CJX-xv3UxmCn"
   },
   "source": [
    "<font color=red>Write your answers here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKSi_6Qkxo4D"
   },
   "source": [
    "### 1.3 Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JjqaOSDMxuRc"
   },
   "source": [
    "In the previous sections, we implemented an MLP that accepts an input $x$ and propagates it forward and produces an output $\\hat{y}$. The next step in implementing our MLP is to see how good our network's output $\\hat{y}$ is compared to the target output $y$! This is where the loss function comes in. This function gets $y$ and $\\hat{y}$ as its inputs and returns a scaler as its output. This scaler indicates how good current parameters of the network are. <br>\n",
    "the choice of this function depends on the task, e.g regression or binary classification. Since you'll be doing a multiclass classification later in this assignment, let's implement the cross-entropy function. Cross-entropy is the function mostly used for classification tasks but to use it in a multiclass setting, the network's outputs must be passed through a softmax activation function and the target output must be in one-hot encoded format.<br>\n",
    "<center>\n",
    "<img src=https://miro.medium.com/max/1838/1*lqHLeRpETQL0Z5lk1euaJA.png width=\"500\" align=\"center\">\n",
    "</center>\n",
    "<br>\n",
    "$$Softmax(\\hat{y})_i =  \\frac{e^{\\hat{y}_i}}{\\sum^{C}_j e^{\\hat{y}_j}} $$ <br>\n",
    "$$ Cross Entropy(y, \\hat{y}) = -\\sum_i^C {y_i log(Softmax(\\hat{y})_i)}$$\n",
    "Where $y$ and $\\hat{y}$ are two one-hot encoded vectors. $y$ is a single target label and $\\hat{y}$ is a single output.<br>\n",
    "Now let's first implement the softmax activation function! Note that the above formulas are for a single sample, however you should implement the batch version!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6WGCohQUivOS"
   },
   "outputs": [],
   "source": [
    "def softmax(y_hat):\n",
    "  \"\"\"\n",
    "    Apply softmax to the inputs\n",
    "  Args:\n",
    "    y_hat: A numpy array of shape (b, out_dim) where b is the batch size and out_dim is the output dimension of the network(number of classes) \n",
    "    \n",
    "  Returns:\n",
    "    soft_out: A numpy array of shape (b, out_dim)\n",
    "               \n",
    "  \"\"\"\n",
    "  \n",
    "  ########################################\n",
    "  #     Put your implementation here     #\n",
    "  ########################################\n",
    "\n",
    "  return soft_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bRsstrQQHXYu"
   },
   "source": [
    "Now implement the categorical cross-entropy function (\"categorical\" refers to multiclass classification). Note that the inputs are in batches, so the loss of a batch of samples will be the average of losses of samples in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dt9YcJAr8ADF"
   },
   "outputs": [],
   "source": [
    "def categorical_cross_entropy(y, y_soft):\n",
    "  \"\"\"\n",
    "    Compute the categorical cross entropy loss\n",
    "  Args:\n",
    "    y: A numpy array of shape (b, out_dim). Target labels of network.\n",
    "    y_soft: A numpy array of shape (b, out_dim). Output of the softmax activation function\n",
    "    \n",
    "  Returns:\n",
    "    loss: A scaler of type float. Average loss over a batch.\n",
    "\n",
    "  Hint: Use np.mean to compute average loss of a batch\n",
    "               \n",
    "  \"\"\"\n",
    "\n",
    "  ########################################\n",
    "  #     Put your implementation here     #\n",
    "  ########################################\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQyCUJ0jSRUf"
   },
   "source": [
    "Great! You have implemented both softmax and categorical cross-entropy functions. Now instead of applying softmax activation function to the output layer of the MLP and then using categorical cross-entropy as loss function, we can merge these two steps and make a softmax categorical cross-entropy loss function and use linear activation function in the output layer! The reason behind this is that the gradient of the softmax categorical cross-entropy loss with respect to the MLP's output is efficiently calculated as:\n",
    "<br>\n",
    "\n",
    "$$ Softmax(\\hat{y}) - y$$\n",
    "\n",
    "for a single sample. Here $\\hat{y}$ is the MLP's output and $y$ is the target output (labels).<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MkZZovW27k3T"
   },
   "source": [
    "Now let's implement the softmax categorical cross-entropy function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VzbOtjnJMohT"
   },
   "outputs": [],
   "source": [
    "def softmax_categorical_cross_entropy(y, y_hat, return_grad=False):\n",
    "  \"\"\"\n",
    "    Compute the softmax categorical cross entropy loss\n",
    "  Args:\n",
    "    y: A numpy array of shape (b, out_dim). Target labels of network.\n",
    "    y_hat: A numpy array of shape (b, out_dim). Output of the output layer of the network\n",
    "    return_grad: If True return gradient of the loss with respect to y_hat. If False just return the loss\n",
    "    \n",
    "  Returns:\n",
    "    loss: A scaler of type float. Average loss over a batch.\n",
    "               \n",
    "  \"\"\"\n",
    "  \n",
    "  y_soft = softmax(y_hat)\n",
    "  \n",
    "  if not return_grad:\n",
    "    loss = categorical_cross_entropy(y, y_soft)\n",
    "    return loss\n",
    "  else:\n",
    "    loss_grad = (y_soft - y)/y.shape[0]\n",
    "    return loss_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CenxHCNZD-5V"
   },
   "source": [
    "### 1.4 Back-Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "66l6gCb9EDvW"
   },
   "source": [
    "After calculating the loss of the MLP, we need to propagate this loss back to the hidden layers in order to calculate the gradient of the loss with respect to the weights and biases of the network. The algorithm used to calculate these gradients is called back-propagation or simply backprop. Backprop uses chain rule to compute the gradients of the network parameters. Now let's go over the steps of this algorithm (This is the fully matrix-based version):\n",
    "- calculate gradient of the loss with respect to $\\hat{y}$\n",
    "<br> $g \\longleftarrow \\nabla_\\hat{y} Loss$ \n",
    "- for each layer $L$ starting from the ouput layer: <br>\n",
    "&emsp;&emsp; $g \\longleftarrow g \\odot f^\\prime(weightedInput^{(L)})$ &emsp; ($weightedInput^{(L)}$ is the weighted input of $L$th layer and $f$ is the activation function)<br>\n",
    "&emsp;&emsp; $\\nabla_{b^{(L)}}Loss \\longleftarrow \\sum_i^{batch} {g_i}$ <br>\n",
    "&emsp;&emsp; $\\nabla_{w^{(L)}}Loss \\longleftarrow output^{(L-1)T}g$ &emsp; ($output^{(L-1)}$ is the output of $(L-1)$th layer ) <br>\n",
    "&emsp;&emsp; $g \\longleftarrow gw^{(L)T}$\n",
    "\n",
    "Check [this](http://neuralnetworksanddeeplearning.com/chap2.html) for a detailed explanation of the back-propagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r-bV6UF0mJMC"
   },
   "source": [
    "Now implement the back-propagation algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-pJJYYHgyKe"
   },
   "outputs": [],
   "source": [
    "def mlp_gradients(mlp, loss_function, x, y):\n",
    "  \"\"\"\n",
    "    Compute the gradient of loss with respect to mlp's weights and biases\n",
    "  Args:\n",
    "    mlp: An object of MLP class\n",
    "    loss_function: A function used as loss function of the MLP\n",
    "    x: A numpy array of shape (batch_size, input_dim). The MLP's input\n",
    "    y: A numpy array of shape (batch_size, num_classes). Target labels\n",
    "    \n",
    "  Returns:\n",
    "    gradients: A list of dictionaries {'w': dw, 'b': db} corresponding to the dictionaries in mlp.parameters\n",
    "        dw is the gradient of loss with respect to the weights of the layer \n",
    "        db is the gradient of loss with respect to the bias of the layer \n",
    "               \n",
    "  \"\"\"  \n",
    "\n",
    "  gradients = []\n",
    "\n",
    "  ### get the output of the network\n",
    "  y_hat = mlp.activations[-1]\n",
    "  num_layers = len(mlp.parameters)\n",
    "\n",
    "  ### compute gradient of the loss with respect to network output\n",
    "  g = loss_function(y, y_hat, return_grad=True)\n",
    "\n",
    "  ### You'll need the input in the last step of backprop so let's make a new list with x in the beginning\n",
    "  activations = [x] + mlp.activations \n",
    "  \n",
    "  for i in reversed(range(num_layers)):\n",
    "    ########################################\n",
    "    #     Put your implementation here     #\n",
    "    ########################################\n",
    "    \n",
    "    \n",
    "  return gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UPjKQSS6mi_Q"
   },
   "source": [
    "### 1.5 Optimizaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fe0IJelEmmXw"
   },
   "source": [
    "Now that we've computed the gradients of the parameters of our MLP, we should optimize these parameters using the gradients in order for the network to produce better outputs. <br> \n",
    "Gradient descent is an optimizaion method that iteratively moves the paramters in the oposite direction of their gradients. Below is the update rule for gradient descent:\n",
    "<br><br>\n",
    "$$ w \\leftarrow w - \\alpha \\nabla_wLoss$$ \n",
    "<br>\n",
    "Where $\\alpha$ is the learning rate hyperparameter.<br>\n",
    "There are three main variants of gradient descent: stochastic gradient descent, mini-batch gradient descent and batch gradient descent. <br>\n",
    "Mini-batch gradient descent is the most used variant in practice and that's what we'll use in this assignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOx9iK3G4bef"
   },
   "source": [
    "Let's perform a step of gradient descent on a simple MLP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mxleIKcO4ama"
   },
   "outputs": [],
   "source": [
    "x = np.random.normal(size=(16, 10))\n",
    "y = np.eye(16)\n",
    "lr = 0.1\n",
    "### Define the mlp \n",
    "mlp = MLP(x.shape[-1])\n",
    "mlp.add_layer(16)\n",
    "mlp.add_layer(8)\n",
    "mlp.add_layer(y.shape[-1])\n",
    "### compute mlp's output\n",
    "y_hat = mlp.feed_forward(x)\n",
    "### print current loss\n",
    "print(\"loss before gradient descent: \", softmax_categorical_cross_entropy(y, y_hat))\n",
    "### Compute gradients of the mlp's parameters \n",
    "grads = mlp_gradients(mlp, softmax_categorical_cross_entropy, x, y)\n",
    "### perform gradient descent\n",
    "mlp.parameters = [{'w':p['w']-lr*g['w'], 'b':p['b']-lr*g['b']} for g, p in zip(grads, mlp.parameters)]\n",
    "### compute mlp's output again after gradeint descent\n",
    "y_hat = mlp.feed_forward(x)\n",
    "### print loss after gradient descent\n",
    "print(\"loss after gradient descent: \", softmax_categorical_cross_entropy(y, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JFsBdy9v7iFs"
   },
   "source": [
    "__Question__: Do gradient descent steps always decrease the loss? why?   (Hint: toy with the learning rate in the axample above!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hYYgkfw58fm5"
   },
   "source": [
    "<font color=red>Write your answers here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7mtWfd88kbA"
   },
   "source": [
    "Instead of using gradient descent, we'll be using an extention of it called gradient descent with momentum. So instead of updating the parameters based only on current gradients, we take into account the gradients from previous steps! This way, parameter updates will have lower variance and convergence will be faster and smoother. \n",
    "$$ v \\leftarrow \\gamma  v - \\alpha \\nabla_wLoss$$ \n",
    "$$ w \\leftarrow w + v$$\n",
    "Where $w$ denotes mlp's weights and $v$ is called velocity which is basically a weighted average of all previous gradients.<br>\n",
    "Here $\\gamma$ determines how fast effects of the previous gradients fade and $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3s9BFOPoMDoL"
   },
   "source": [
    "Now let's implement the SGD class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O8EW1X-FNmfA"
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "\n",
    "  def __init__(self, lr=0.01, momentum=0.9):\n",
    "    \"\"\"\n",
    "  Args:\n",
    "    lr: learning rate of the SGD optimizer\n",
    "    momentum: momentum of the SGD optimizer\n",
    "\n",
    "    Hint: velocity should be a list of dictionaries just like mlp.parameters\n",
    "               \n",
    "  \"\"\" \n",
    "\n",
    "    self.lr = lr\n",
    "    self.momentum = momentum\n",
    "    ### initialize velocity\n",
    "    self.velocity = []\n",
    "  \n",
    "  def step(self, parameters, grads):\n",
    "\n",
    "    \"\"\"\n",
    "    Perform a gradient descent step\n",
    "  Args:\n",
    "    parameters: A list of dictionaries {'w': weights , 'b': bias}. MLP's parameters. \n",
    "    grads: A list of dictionaries {'w': dw, 'b': db}. gradient of MLP's parameters. Basically the output of \"mlp_gradients\" function you implemented!\n",
    "    \n",
    "  Returns:\n",
    "    Updated_parameters: A list of dictionaries {'w': weights , 'b': bias}. mlp's parameters after performing a step of gradient descent. \n",
    "               \n",
    "  \"\"\"\n",
    "\n",
    "    ########################################\n",
    "    #     Put your implementation here     #\n",
    "    ######################################## \n",
    "\n",
    "    \n",
    "\n",
    "    return Updated_parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BDqMMm-pNlVF"
   },
   "source": [
    "## 2. Classifying Kannada Handwritten Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PpTgQUZ7Ofcw"
   },
   "source": [
    "In this part of the assignment, you'll use the MLP you implemented in the first part to classify Kannada handwritten digits!<br> This dataset consists of 60000 images of handwritten digits in Kannada script.<br>\n",
    "You can check [this](https://github.com/vinayprabhu/Kannada_MNIST) github repository for more information about the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Please Unzip the kannada file first__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k9vIApVkPgMl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oXQcevYVPpxX"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GtLGkfCJQTTb"
   },
   "source": [
    "As you can see, the first column of the dataframe is the label, and the rest of the columns are the pixels. Let's put the dataset in numpy arrays. Also, we must normalize the pixel values to [0,1] range to help the convergence of our MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDDtw13gQs--"
   },
   "outputs": [],
   "source": [
    "x = train.values[:, 1:]/255.\n",
    "y = train.values[:, 0]\n",
    "plt.imshow(x[10000].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJgwvpb5Xec4"
   },
   "source": [
    "As we are doing a multiclass classification, the labels must be in one-hot encoded format. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AB8TavCl7TSd"
   },
   "outputs": [],
   "source": [
    "def one_hot_encoder(y):\n",
    "\n",
    "  y = y.reshape(-1)\n",
    "  num_samples = y.shape[0]\n",
    "  max_label = np.max(y)\n",
    "  one_hot = np.zeros((num_samples, max_label+1))\n",
    "  one_hot[np.arange(num_samples),y] = 1\n",
    "  \n",
    "  return one_hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T3c9FP4ZdZvv"
   },
   "source": [
    "Now let's transform the labels into one-hot encoded format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_DuFO6g3dWgC"
   },
   "outputs": [],
   "source": [
    "y = one_hot_encoder(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KDWfbIPzaUTB"
   },
   "source": [
    "We've implemented the _get_mini_batches_ function below. This function transforms the dataset into multiple batches. We need this function because we'll be doing mini-batch gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQwrWQdR8wCd"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_mini_batches(x, y, batch_size, shuffle=True):\n",
    "\n",
    "  idx = list(range(len(x)))\n",
    "  np.random.shuffle(idx)\n",
    "  steps = math.ceil(len(x)/batch_size)\n",
    "  x, y = x[idx, :], y[idx, :]\n",
    "  for i in range(steps):\n",
    "    yield (x[i*batch_size: (i+1)*batch_size], y[i*batch_size: (i+1)*batch_size])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4iQ_J2VXf2M_"
   },
   "source": [
    "Evaluation metrics are used to measure the performance of a model after training. The choice of this metric depends on factors like the nature of the task (e.g classification or regression) or a dataset's characteristics (e.g class imbalance). For multiclass classification with balanced classes, accuracy is a reasonable choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eiu967nZiU76"
   },
   "source": [
    "We've implemented the accuracy function in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0JWfurLbvjDL"
   },
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "\n",
    "  return np.mean(np.argmax(y, axis=-1)==np.argmax(y_hat, axis=-1))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_RcB5fPVibUG"
   },
   "source": [
    "Now let's split the dataset into train and validatoin sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bWUJ-g8bT_JC"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B7utzA1JimT0"
   },
   "source": [
    "Everything is now ready for training our MLP! Create your MLP model in the cell bellow. The choice of the number of layers, their sizes and their activation functions is up to you.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nHhoNHa3vz4V"
   },
   "outputs": [],
   "source": [
    "\n",
    "mlp = MLP(x_train.shape[-1])\n",
    "\n",
    "########################################\n",
    "#     Put your implementation here     #\n",
    "########################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v0-8HUbYkXCQ"
   },
   "source": [
    "Let's set some hyper-parameters. Feel free to change these hyper-parameters however you see fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3YZeR-pkBI-"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "Batch_size = 1024\n",
    "sgd_lr = 0.1\n",
    "sgd_momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6RF5pttlG8l"
   },
   "source": [
    "Now let's train the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WOUEaeTxjTIE"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "### Defining a optimizer\n",
    "optimizer = SGD(lr=sgd_lr, momentum=sgd_momentum)\n",
    "\n",
    "train_loss, val_loss, train_accs, val_accs = [], [], [], []\n",
    "\n",
    "for i in range(epochs):\n",
    "  mini_batches = get_mini_batches(x_train, y_train, Batch_size)\n",
    "  for xx, yy in tqdm_notebook(mini_batches, desc='epoch {}'.format(i+1)):\n",
    "\n",
    "    ### forward propagation\n",
    "    mlp.feed_forward(xx)\n",
    "    ### compute gradients\n",
    "    grads = mlp_gradients(mlp, softmax_categorical_cross_entropy, xx, yy)\n",
    "    ### optimization\n",
    "    mlp.parameters = optimizer.step(mlp.parameters, grads)\n",
    "    \n",
    "  y_hat = mlp.feed_forward(x_train)\n",
    "  y_hat_val = mlp.feed_forward(x_val)\n",
    "  val_loss.append(softmax_categorical_cross_entropy(y_val, y_hat_val))\n",
    "  train_loss.append(softmax_categorical_cross_entropy(y_train, y_hat))\n",
    "  train_acc = accuracy(y_train, y_hat)*100\n",
    "  val_acc = accuracy(y_val, y_hat_val)*100\n",
    "  train_accs.append(train_acc)\n",
    "  val_accs.append(val_acc)\n",
    "  print(\"training acc: {:.2f} %\".format(train_acc))\n",
    "  print(\"test acc: {:.2f} %\".format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rzrzvDPXnLlI"
   },
   "source": [
    "Let's visualize accuracy and loss for train and validation sets during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R_rB9k9d2f81"
   },
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(train_loss))), train_loss, label='train')\n",
    "plt.plot(list(range(len(val_loss))), val_loss, label='val')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cP3Rgp99o-f7"
   },
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(train_accs))), train_accs, label='train')\n",
    "plt.plot(list(range(len(val_accs))), val_accs, label='val')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nu0QS_stnsAX"
   },
   "source": [
    "__Question__: Looking at loss and accuracy plots, how would you describe your model in terms of bias and variance?\n",
    "\n",
    "For bias and variance you can check <a href=\"https://medium.com/@itbodhi/bias-and-variance-trade-off-542b57ac7ff4\"> This link</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFA-4VdmoL4E"
   },
   "source": [
    "<font color=red>Write your answers here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP using tensorflow and keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you should use keras and tensorflow to implement the exact network that you built in previous section. use the exact paramerters and then classify kannada and report the accuracy. at the end compare the resualt of 2 model you biult and explain it. \n",
    "\n",
    "If you need more information about keras and implementation you can check <a href=\"https://www.machinecurve.com/index.php/2019/07/27/how-to-create-a-basic-mlp-classifier-with-the-keras-sequential-api/\"> This link</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "#     Put your implementation here     #\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "assignment_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
